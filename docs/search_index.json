[["a-primer-in-statistics.html", "A A primer in statistics A.1 Populations and samples A.2 Three types of variability: of the sample, of the population and of the estimate. A.3 Confidence intervals: a way of precisely representing uncertainty A.4 The \\(t\\)-distribution A.5 Hypothesis testing A.6 Size effects vs hypothesis testing.", " A A primer in statistics Based on the very helpful revision chapter in Modern Statistics for the Life Sciences, Alen Grafen and Rosie Hails, OUP. A.1 Populations and samples It is rarely possible to get an exact answer to a question. Normally we have to make do with an estimate, and this may vary from a rough estimate to a more precise one. One of the first tasks of statistics is to state this in more precise terms. Suppose, for example, we wanted to know the average height of adult women between the ages of 25 and 35 in the United Kingdom. It is quite impossible to measure the height of every single woman of that age. Instead we must content ourselves with taking a sample, finding the average height of women within that and hoping that it is representative of the whole population. A sample is a random selection from within a population of interest. For this to be the case the population of interest needs to be precisely defined, since our sampling strategy will depend on this. In the case above, our population of interest is not people, or women, or adult women, or adult women between the ages of 25 and 35, but adult women between the ages of 25 and 35 in the United Kingdom. We would thus choose our sample randomly from among just these women, and ignore all other people on the planet. Having taken our sample we can compute the sample mean \\(\\bar{y}\\) (pronounced y-bar) by summing over the \\(y_i\\) and dividing by the number of values: \\[ \\bar{y}=\\frac{y_1+y_2+\\dots + y_n}{n}=\\frac{\\Sigma y_i}{n} \\] In this equation, we suppose that we have a sample of size \\(n\\) women, with each individual woman denoted by \\(y_1, y_2\\dots y_n\\) or in general for any individual by \\(y_i\\) where the subscript \\(i\\) runs from \\(1\\) to \\(n\\). To find the mean value we first sum the \\(y_i\\) from \\(i=1\\) to \\(i=n\\) and then divide the result by \\(n\\). A compact way to denote the sum is to use the summation sign \\(\\Sigma\\) (sigma), as we do in the right-hand term of the equation This is our estimate of the true population mean, \\(\\mu\\). It is similar when we do an experiment. Measurements in an experiment inevitably involve error, and so we can think of the data readings we actually take as being a sample from the population of all the readings that could have occurred. The mean value that we get at the end from our data is thus an estimate of the true mean \\(\\mu_\\text{A}\\), which we would only have obtained if there was no error involved in the experiment. A.2 Three types of variability: of the sample, of the population and of the estimate. A.2.1 Variability of the sample As well as its mean, we would like to know how variable our sample is. This gives us an idea as to how variable the population is and as to what the variability of the next sample will be. The sample variability will fluctuate from one sample to the next drawn from the same population, but will not systematically increase or decrease as the sample size changes. The size of the fluctuations will however reduce as the sample size increases, so that the sample variability more and more closely resembles the variability of the population. Two samples can have the same mean but very different variability. For example if the results for a class of n = 30 students for a test in maths ranged from 40 to 70 while those for a test in English ranged from 50 to 60, then both might have a mean of 55, but, clearly, results in maths would be more variable than those in English, as we see in Figure A.1 Figure A.1: Scores in Maths and English tests. Both sets of scores have the same mean value of 55, but the maths scores have greater variance than the English scores For any individual score in either test, we can calculate its deviation from the mean of the score for that test, where \\[ \\text{deviation} = \\text{datapoint} - \\text{mean} \\] If we sum the deviations of the scores from each test from their respective means, we would find that the absolute value of these deviations tend to be bigger for the maths test than for the English test. For both tests, however, the sum of the deviations would be zero, because of the definition of the mean as the central point, but if the deviations are squared and summed, we then get a measure of the variability of each data set around its mean. Scores Mean Sum of deviations Sum of squared deviations Variance Standard deviation \\(\\bar{y}\\) \\(s^2\\) \\(s=\\sqrt{s^2}\\) English scores \\(\\frac{1}{n}\\Sigma y_i=55\\) \\(\\Sigma \\left(y_i-\\bar{y}\\right)=0\\) \\(\\Sigma (y_i-\\bar{y})^2=638\\) \\(\\frac{1}{n-1}\\Sigma (y_i-\\bar{y})^2=\\frac{638}{29}=22.0\\) \\(\\sqrt{22.0}=4.7\\) maths scores \\(\\frac{1}{n}\\Sigma y_i=55\\) \\(\\Sigma \\left(y_i-\\bar{y}\\right)=0\\) \\(\\Sigma (y_i-\\bar{y})^2=2219\\) \\(\\frac{1}{n-1}\\Sigma (y_i-\\bar{y})^2=\\frac{2219}{29}=76.5\\) \\(\\sqrt{76.5}=8.75\\) In this case, a comparison of the sums of squares is valid, since the two data sets have the same size. In general though, a larger data set will have a larger sum of squares, so for a valid comparison between unequally sized data sets, a measure that is independent of the size of the data set is required. To get this, all we need to do is take account of the sample size. We used n data points to define the mean, then the same n data points, plus the mean itself, to define the variability around the mean. But from the way in which the mean is calculated, the deviations must sum to zero. This means that we have only n-1 independent pieces of information about how the sample varies around the mean. Hence, our final measure of the variability of a data set, which we call the variance and denote as \\(s^2\\), is found by dividing the sum of squared deviations by n-1, not by n. \\[ s^2=\\frac{\\Sigma{\\left(y_i-\\bar{y}\\right)^2}}{n-1} \\] The number of independent pieces of information that contribute to the calculation of a statistic is called the degrees of freedom. Often, we would like a measure of variability that has the same units as the data itself. The variance does not, but we remedy that by taking its square root to find the standard deviation \\(s\\) of the data set. A.2.2 Variability of the population Just as we cannot know the true mean \\(\\mu\\) of a population, but can only estimate it from the mean \\(\\bar{y}\\) of a sample that we draw from that population, so we cannot know the true variance of a population. Nevertheless, it is useful to define it, and it is frequently referred to as \\(\\sigma^2\\) (“sigma squared”). Our best estimate of it is our sample variance \\(s^2\\). A definition of it is that it is the expected squared deviation around the true mean for all individuals in the population. In general, the variance of a sample will be approximately equal to that of the population, which is why we can use it as an estimate of the population variance. If we took sample after sample we would find that we got a different sample variance each time, all of them dotting randomly around the true value for the population. What we would not find is that the sample variance would systematically change if the sample size changed. In particular, it would not get systematically smaller as the sample size increased. A.2.3 Variability of the estimate Having obtained our estimate \\(\\bar{y}\\) of the true mean \\(\\mu\\) of a population, we would like to know how accurate it is. To answer this we will briefly discuss Normal distributions. A.2.3.1 The Standard Normal Distribution Many continuous attributes (eg weight, height, width) of a population are scattered around a mean value in such way that, if you plotted a histogram of the values, it would have a shape that is approximately bell-shaped and symmetric and well described by a ‘normal’ distribution (the reasons for this are interesting, but beyond the scope of this document). Such a distribution is described by two parameters - its mean (middle value) and its variance (spread). A standard normal distribution is simply a normal distribution with a mean of 0 and a variance (and therefore a standard deviation, which is the square root of the variance) of 1. Such a distribution is sometimes referred to as a \\(Z\\) distribution. Any normal distribution can be converted to a standard normal distribution by doing two things, as illustrated in the following example: Suppose a variable \\(Y\\) follows a normal distribution, with mean 5 and standard deviation 2. First we subtract the mean from every value. This will have the effect of moving the whole distribution leftwards on the x-axis by 5 units, the mean value of Y, so that it is centred on 0. Then, we divide each value by 2, the standard deviation of Y. This will have the effect of squishing the distribution inwards, giving it a new standard deviation of 1. The result will be a standard normal, centred at 0, with standard deviation 1. The process of carrying out these two operations is known as standardising, and is illustrated in Figure A.2. Figure A.2: Standardising a normal distribution In summary, in order to convert a variable Y that is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), to a standard normal z, we subtract the mean then divide by the standard deviation: \\[ z=\\frac{Y-\\mu}{\\sigma} \\] Why would we want to do this? The answer is that the standard normal is an example of a probability density function (or pdf for short). Such functions have particular properties that are useful to us as scientists. In particular, it is straightforward to calculate what proportion of any set of observations described by a pdf fall within or beyond a certain number of standard deviations from the mean. The main thing to understand is that the area under a pdf between any two values of the x-axis tells us the probability that the random variable falls between those values. In particular, the total area under a standard normal, as for any pdf, is 1, since it is a certainty that the random variable it describes takes some value or other. The area under it to the right of zero and the area under it to the left of zero are both 0.5, since the distribution is symmetric about zero. This tells you that if you had a random variable that was described by a standard normal, then there would be a 50% chance that it was positive, and a 50% chance that it was negative. In general, if you take a random individual from a population and measure the value of some attribute (such as its height) that is well described by a normal distribution, then there would be a 50% chance that the value for this individual is less than the population mean, and a 50% chance that it is greater. The area under the distribution beyond a distance roughly two standard deviations (actually, 1.96) either side of the mean totals 0.05, or 5% of the total area under the curve. This means that if, again, we have a population for which some attribute is well described by a normal distribution, then roughly 95% of individuals will have a value of that attribute that falls within two standard deviations of the mean, and roughly 5% of them will fall beyond that. Or, put another way, if you took a random individual from the population, there is a 95% chance that its value for this attribute would be within about 2 standard deviations of the mean, and a 5% chance that it would be more than about 2 standard deviations greater or less than the mean. This is illustrated in Figure A.3 Figure A.3: Aspects of a standard normal Of particular practical importance, if a data set is normally distributed, then about 68% of the observations fall with one standard deviation of the mean, 95% fall within 1.96 standard deviations, about 96% fall with two standard deviations, and about 99.7% (ie practically all of them) fall within three standard deviations. Figure A.4 illustrates this. Figure A.4: Proportion of normally distributed observations within one, two or three standard deviations of the mean So much for an ideal normal distribution. A real data set drawn from a population that is approximately normally distributed would have some scatter, the more so the smaller the size of the sample. For such a sample we would find that approximately two thirds of the data set is within one standard deviation of the mean, approximately 95% of it is within two standard deviations and pretty much all of it is within three standard deviations. A.2.3.2 Accuracy of the estimate We are interested in the population. We want to know its true mean \\(\\mu\\), but what we have as our best estimate of this is the mean \\(\\bar{y}\\) of a sample of size n that we drew from the population. If we took another sample from the population of the same size, we would get a different sample mean, and so on again and again, if we had the time and resources to repeatedly take sample after sample. So our sample mean is itself a random variable \\(\\bar{Y}\\), drawn from a population of all possible sample means. If we drew samples of the same size n many times from our population of interest, the means \\(\\bar{y}\\) of these samples would themselves form a distribution, the so-called sampling distribution, and the mean of this we would hope, would be the true mean \\(\\mu\\) of the population. The bigger the variance \\(\\sigma^2\\) of the population, the more we would expect our estimate to differ from the true mean, and the less variable the population was, the closer we would expect our estimate to be to the true mean. Similarly, if we took a large sample then our estimate is likely to be closer to the true mean than if we took a small sample. If we take these observations together, what we find is that the variance of the distribution of our estimates is \\(\\sigma^2/n\\), and so the standard deviation of our estimate is the square root of this ie \\(\\frac{\\sigma}{\\sqrt{n}}\\). This is sometimes called the standard error of the mean. This gives us an idea of how precise our sample mean is as an estimate of the true mean. In most figures that you come across in published papers where error bars are used, these error bars will be ± one standard error of the mean. You may also come across error bars that are ± one standard deviation of the sample and yet others that show the (typically) 95% confidence interval for an estimate. Whether you show ± one standard deviation or ± one standard error of the mean depends on the story you are trying to tell. If you want to be descriptive and give the reader an idea as to the variability of a population, you would use error bars that are ± one standard deviation, but if you want the reader to be able to infer how close one population is to another then you are more likely to want to use error bars that are ± one standard error of the mean. A.2.3.3 Example Suppose the population of grey seals around the coast of south west England includes 10,000 adult females, whose weights are normally distributed and in the range 100 - 190 kg. Let this be our ‘population of interest’. The weights of individuals in this population are approximately normally distributed with a mean value of 145 kg, and a standard deviation of 15 kg. Note that in reality we would not know the mean or standard deviation of this population, or how many seals there were in total or whether the weights of adult females within it were normally distributed (or distributed any other way). A histogram of the weights of the entire population would look something like Figure A.5: Figure A.5: Histogram of the weights of the entire population of adult grey seals in South West England. #| In reality, we would never be able to measure this, but if we could, it would very likely closely approximate a normal distribution, as shown here. Suppose we wanted to know the mean and standard deviation of the weights of adult female grey seals in this population. Clearly, we could not find the true value since that would require weighing every seal in the population, which is impossible, but we could estimate the values by weighing all seals in a manageable sample that we hope is representative of the whole population. Suppose our sample were 100 randomly chosen adult female seals. In reality, that would probably be the only sample we could get, and so our estimates \\(\\bar{y}\\) and s of the true mean \\(\\mu\\) and true standard deviation \\(\\sigma\\) respectively would be based entirely on that one sample. To get an idea of how accurate our estimate is, imagine we could measure such samples of 100 seals randomly selected from the population many, many times over. Each sample would have a slightly different mean. In Figure A.6 below, let us plot the distribution of some of those samples and superpose on top of them the ‘normal distribution’ curve that we know is a good representation of the weights of adult females in the whole population. (We know this because this is a simulation. In truth, we wouldn’t.) For each sample, we display its mean \\(\\bar{y}\\) and standard deviation s. Figure A.6: Histograms of a selection of samples of 100 seals, each drawn from the same population. Superposed on each one is a normal distribution Notice how all these sample distributions have roughly the form of a normal distribution but that each one is in detail different from the others. This is the reality of sampling from a population - every sample will be different - but not, usually, unrecognisably different. All those shown have roughly the same mean, shown by the dashed line, roughly the same standard deviation and roughly the same shape. Note too that these samples are drawn from a population (we happen to know, because we created it!) whose mean value \\(\\mu\\) is 145 and for which the standard devation \\(\\sigma\\) is 15. In an actual study, we would have taken, most likely, just one sample, which could have been any of those you see above. The mean \\(\\bar{y}\\) and standard deviation \\(s\\) of that sample would have been our best estimate of \\(\\mu\\) and \\(\\sigma\\). Below in Figure A.7 we contrast histograms of the population, a single sample of size 100 drawn from the population, and the so-called sampling distribution. That is, the distribution of the means of many samples of size 100 drawn from the population. Of these, the middle one, that of a single sample, is the only one we could get in practice. Figure A.7: The distribution of seal weights across the whole population, a sample drawn from that population, and the sampling distribution of sample means. In this case, the weights of the population are very close to being normally distributed. The mean is 145 kg and the standard deviation is 15 kg. You can see that the whole population has a weight within two or three standard deviations of the mean. In practice, we do not usually know either that the parameter of interest, in this case weight, definitely is normally distributed, or the mean and standard deviation of that distribution. The weights of one sample of size 100 drawn from the population are also approximately normally distributed with a mean and standard deviation approximately equal to the that of the population. In practice, we might often only have only this one sample, so these would be our best estimates of the population mean and standard deviation and our judgement as to whether the population was normally distributed would be based on this one sample distribution alone. With small samples, it can often be hard to tell, just by looking at this histogram, whether the data have been drawn from a population that is normally distributed. (In practice, we might also use other considerations - such as whether the data were a simple random sample and whether there were no outliers, and so on.) The sampling distribution is in practice a hypothetical distribution, since we cannot normally take many samples, each here of size 100, find the mean of each and plot the distribution of these. But if we could, this is what we would get. The mean of the one sample that we actually got is somewhere within this distribution and the true mean of the population is at the centre of it. A very interesting and useful thing about this distribution is that it will very likely be normally distributed, even if the population distribution was not, provided the sample size is large enough, and it is narrower than the population distribution. The larger the sample size, the narrower it is. These are handy facts, since they together mean that its width gives us an idea of the precision of our sample mean as an estimate of the the true mean. Some facts of interest about the sampling distribution are: it is normally distributed (and, if the sample sizes are large enough and the samples independent of each other, it probably would be normally distributed even if the underlying distribution of the population were not a normal distribution.) This is a remarkable fact. its mean is the true mean \\(\\mu\\) of the population. its standard deviation is narrower than that of the population as a whole or of one sample. If the standard deviation of the population is \\(\\sigma\\), where \\(\\sigma = 15\\)kg in this case, and the samples each had size n, where n=100 in this case, then the standard deviation of this sampling distribution is \\(\\frac{\\sigma}{\\sqrt{n}}\\). So in this case, the standard deviation of this distribution is 1/10th that of the underlying population. if our sample size had been bigger, the sampling distribution would have been even narrower and so our estimate of the true mean would have been more precise. That is the benefit of having a bigger sample size. Now, here is the really interesting thing about this distribution. It tells us about the precision of our estimate \\(\\bar{y}\\) that we got from our one sample of 100 seals of the true population mean \\(\\mu\\). Look at that sampling distribution. The true mean is somewhere in there, as is our sample mean. So whatever this value \\(\\bar{y}\\) is that we got from our sample, we know it is within the width of this distribution of the true mean. And how wide is this distribution? Well, since it reliably has the shape of a normal distribution, we know the answer to that. Roughly 95% of the values on this distribution are within two of its standard errors of the middle value \\(\\mu\\). This standard error, remember, is \\(\\frac{\\sigma}{\\sqrt{n}}\\), where \\(\\sigma\\) is the population standard deviation, our best estimate of which is the standard deviation \\(s\\) of our sample. Our sample mean \\(\\bar{y}\\) is our best estimate of this middle value, so we end up being able to say something like the following: \\[ \\text{...the true mean = }\\bar{y}{\\text{ (our sample mean)}}\\pm 2\\times \\frac{s{\\text{ (our sample standard deviation)}}}{\\sqrt{n}} \\] We would call this range the 95% confidence interval for the thing we wanted to measure - in this case, the mean weight of adult female grey seals in the population of them around the south west of England. What we mean by this is that if we repeatedly took a sample of 100 seals from the population and constructed the confidence interval for the mean in this way, then the true value would be within the interval 95% of the time. Let us explore confidence intervals in more detail… A.3 Confidence intervals: a way of precisely representing uncertainty We know that our estimate \\(\\bar{y}\\) of the population mean \\(\\mu\\) comes from the distribution of all possible \\(\\bar{y}\\) that are distributed around \\(\\mu\\) with a variance of \\(\\frac{\\sigma^2}{n}\\), and thus a standard deviation of \\(\\frac{\\sigma}{\\sqrt{n}}\\). Let us now find the confidence interval from our data. This is the range of possible values for the true population mean (which we don’t know, remember) that cannot be rejected at the 5% significance level. Parameters that have been estimated with great precision will have a narrow confidence interval associated with them, while parameters about which we have less information will have a wide confidence interval. From the properties of the standard normal distribution, we know that 95% of all such \\(\\bar{y}\\) will lie within 1.96 standard deviations of \\(\\mu\\), where the relevant standard deviation is that of the sampling distribution - the distribution of \\(\\bar{y}\\). That means that 5% will not! This is illustrated below in Figure A.8, where, for example, we show the true mean weight \\(\\mu\\) of adult female grey seals in south west England as a dotted line and either side of that, estimates of that obtained as the means from 20 samples, each of 100 seals, with their corresponding 95% confidence intervals. Note how nearly all of these confidence intervals do capture the true mean, but that one (in this case) does not. Figure A.8: If many samples were taken from the same population, and a 95% confidence interval for the mean calculated from each one, about 95% of these would capture the true mean Hence we can say that, for 96% of the time: \\[ \\mu-2\\frac{\\sigma}{\\sqrt{n}} \\lt \\bar{y} \\lt \\mu+2\\frac{\\sigma}{\\sqrt{n}} \\] In practice, by convention, we are interested in a confidence level of 95% rather than 96%. This changes the 2 in the above formula to 1.96 - the confidence level is slightly lower, so the confidence interval is slightly less wide. Further, we would rather instead state a confidence interval for \\(\\mu\\) in terms of \\(\\bar{y}\\), rather than as above, so we rejig the last equation to give: \\[ \\bar{y}-1.96\\frac{\\sigma}{\\sqrt{n}} \\lt \\mu \\lt \\bar{y}+1.96\\frac{\\sigma}{\\sqrt{n}} \\] This is now our 95% confidence interval for data drawn from a normally distributed population: the range of values that the true mean \\(\\mu\\) could take and be consistent with the data at the 95% level. But there is a hitch… A.4 The \\(t\\)-distribution The trouble with the final expression in the previous section, as a way of stating the confidence interval for a parameter such as the mean of some measure of a population, is that it requires that we know \\(\\sigma\\), the true standard deviation of of the population, and we don’t know it exactly. All we have is an estimate of it, ie \\(s\\), the standard deviation of the sample. So there is some uncertainty in our knowledge of \\(\\sigma\\), just as there is in our knowledge of \\(\\mu\\) and this results in our 95% confidence interval for \\(\\mu\\) being somewhat wider than the value given above. The way this extra uncertainty can be accommodated is by modelling our data not by a normal distribution, but by a t-distribution. This is similar to a normal distribution in that it is symmetrical, but it is lower and wider, with heavier tails on either side - which means that extreme values are more likely than for a normal. It is characterised by a centre, a scale and a degrees of freedom parameter df that can range from 1 to \\(\\infty\\) and which is one less than the number of data points in the sample: df =n-1. The precise shape of the t-distribution depends on df. For small df t distributions have very heavy tails, but as the sample size increases and df rises, so the t-distribution becomes taller and narrower and more and more like a normal distribution, until, for df greater than 30 or so, the two are more or less indistinguishable. This reflects the fact that, the more data points we have, the more precise our estimate s becomes of the population standard deviation \\(\\sigma\\). This is illustrated below in Figure A.9, where we see drawn in red t-distributions for df = 1, 3, 10 and 30 (ie sample sizes of 2, 4, 11 and 31) against a standard normal distribution drawn in dark blue. Figure A.9: For low sample sizes (and therefore low degrees of freedom) the t distribution has fatter tails than a normal distribution, but as the sample size increases, it increasingly resembles the normal. For df greater than 30 or so, they are indistinguishable. To calculate the 95% confidence interval now we need to know how many standard deviations of the t-distribution we need to go either side of the mean in order to encompass 95% of the population. We call this the critical t-value \\(t_\\text{crit}\\). For a normal distribution, remember, we had to go 1.96 standard deviations either side in order to do this. For a t-distribution, how far we need to go will depend on the degrees of freedom df. For a low value of df the distribution has fatter tails so we need to go further out, but we need go less far as df increases and the t-distribution becomes narrower until, when df = 30 or so, we need only go as far as we would for a normal distribution, ie 1.96 standard deviations. This is the effect of having a small sample: for such a sample our estimate of the true mean is less precise than if we had a larger sample, so the confidence interval, the range of values in which we are (say) 95% confident that the true value lies, is correspondingly wider. For df = 10, we find that \\(t_\\text{crit}\\) = 2.228 So, now, for small samples, we would write our confidence interval as \\[ \\bar{y}-t_\\text{crit}\\frac{s}{\\sqrt{n}} \\lt \\mu \\lt \\bar{y}+t_\\text{crit}\\frac{s}{\\sqrt{n}} \\] or, put another way, \\[ \\mu= \\text{estimate}\\pm t_\\text{crit}\\times\\text{standard error of the estimate} \\] where the estimate is the mean of our sample, s is the standard deviation of the sample, n is the sample size and, for a 95% confidence interval and df = 10, \\(t_\\text{crit}\\) = 2.228. For other confidence levels or other values of df, \\(t_\\text{crit}\\) would have a different value. A.4.0.1 Pros and cons of using the t-distribution. The t-distribution is widely used as a way of calculating confidence intervals for population parameters from sample estimates. It can be used when the sample size is small, whereas the normal distribution cannot, but it can also be used when the sample size is large, which is really handy. However, it is only valid to use it when the sample comprises independent observations that have been drawn from a population that is normally distributed, and this is not always easy to tell for small samples, just when we would really like to use it. For example the panels in Figure A.10 below show histograms of four samples, each of size 10, all drawn from the same normally distributed population. Would you be able to tell, from looking at these histograms, that this was the case? Figure A.10: Histograms of small samples, drawn from a normally distributed population For such small samples, quantile-quantile plots, or qq-plots as they are often called, are a better visual way to assess normality. In Figure A.11 we show qq-plots for the same four samples. Figure A.11: qq plots of the same small samples. Note that it is easier to tell whether these are close to being straght lines than it is to tell whether the histograms approximate the shape of a normal distribution If the samples are drawn from a normally distributed population, then we expect the data to lie more or less along a straight line in a qq-plot and in each case shown above, they do. Classic cases of samples that are not drawn from a normally distributed population are where the populaton distribution is left- or right-skewed, under-dispersed or over-dispersed. Histograms of these, box-plots and the qq-plots that you getare shown in Figure A.12 below: Figure A.12: Examples of histograms, box plots and qq plots for samples drawn from populations that are a) Normally distributed, b) left-skewed, c) right-skewed, d) under-dispresed and e) over dispersed. A.5 Hypothesis testing Here we lay out the concept of a null hypothesis and the method of testing such a hypothesis. We suppose that a sample mean and variance have been calculated, and that this information has been used to calculate a confidence interval. We can use this same information to test a hypothesis. Suppose our sample was a set of 30 differences between two groups, for example the difference in test scores of a group before and after taking a statistics course. If there was no improvement over the duration of the course, then the mean difference should be zero. If the difference is defined as score after - score before then it is to be hoped that the mean difference is positive. However if the course actually confused the students then the difference could be negative. To start with, we construct a null hypothesis. This usually expresses the conservative, ‘nothing going on’ scenario and states that no effect is expected, but it would be equally valid to state that the true mean takes some non-zero value. In this case: \\[ \\text{H}_0\\text{: There is no difference between the scores, }\\mu=0 \\] The alternative is that there is a difference. Normally, we would not state the direction of this difference, so the alternative hypothesis is phrased as: \\[ \\text{H}_\\text{A}\\text{: }\\mu\\neq0 \\] The main principle of a hypothesis test is that we assume the null hypothesis is true and do not reject it unless there is convincing evidence that it is not true. In that sense it is like a classic court process, in which a defendant is assumed innocent and will be acquitted unless we compile a portfolio of evidence that we would be very unlikely to have if that were true. Note that both the null and alternate hypotheses are phrased in terms of population parameters, since it is the population that we want to know about. The sample that we have drawn from it is just our window onto that. The sample mean \\(\\bar{y}\\) will almost certainly not be zero, and even if it were it would not mean that the true mean \\(\\mu\\), the mean of the population, is zero. So what we do is assume that the null hypothesis is true and calculate the probability that we would have got the data we got, or more extreme data, if that were true. By convention, if this probability falls below 0.05 we reject our assumption of \\(\\text{H}_0\\) being correct. This means that if the null hypothesis is true, there is a probability of 0.05 that we will reject it when we should not. We call this a Type 1 error. Figure A.13: Key values for hypothesis testing, showing the rejection regions for the null hypothesis in blue. Figure A.13 above shows the distribution of our random variable (the mean difference between individual students’ scores before and after a course of study) under the null hypothesis. It is centred at zero, in this case. Our value of \\(\\bar{y}\\), we suppose, is one data point from this distribution. In a hypothesis test we ask how likely it is that this could really be the case. The answer depends on how far from the centre of the distribution our value of \\(\\bar{y}\\) lies. If it is close to 0 then it may well have come from this distribution, but if it is far from it, then we conclude that it is unlikely to have done so. When measuring the distance from 0, it is not the absolute distance that matters, but the number of standard errors of the sampling distribution, which we find by dividing the absolute distance by this standard error. In terms of this unit, this distance is known as the t-statistic: \\[ t_\\text{s}=\\frac{\\bar{y}-0}{\\frac{s}{\\sqrt{n}}} \\] Let us remember that the area between two values under a probability distribution curve is the probability that the random variable described by that pdf takes a value in that range. The probability that the mean of our sample could have been as far or further from 0 than it actually is is equal to the area under the distribution curve beyond that distance from 0, including both sides. We compare this with a critical probability, called the significance level of the test, which we choose but which is conventionally set at 0.05 ie 5% of the total area under the curve. The number of standard errors from 0 at which this happens is a critical value of the t-statistic known as \\(t_\\text{crit}\\). Its value depends on the significance level we choose and on the degrees of freedom ie the sample size. In the end, if our t-statistic is greater than the critical t-value then the probability that we could have got such a value of \\(\\bar{y}\\) from a distribution centered on 0 is less than 0.05. We call this probability a p-value, and so, if \\(p&lt;0.05\\) we decide that the strength of the evidence is such as to allow us to reject the null hypothesis. In summary, the p-value is the probability of obtaining the the data you got, or more extreme data, and thus the t-statistic you got, or an even bigger t-statistic, if the null hypothesis were true. A.5.1 General procedure for a hypothesis test The procedure outlined above can be generalised to include a population mean of any value, not just zero, and to testing other parameters estimated from samples against hypothesised values of those parameters for the population. The procedure can be broken down into these steps: Define the null and alternate hypotheses in terms of population parameters. Plot the data, most likely using a box plot or a histogram. Calculate the sample estimate \\(\\bar{y}\\) of the population parameter. Calculate the standard error \\(s/\\sqrt{n}\\) of this estimate. Determine whether it is appropriate to use a t-test Calculate the t-statistic Calculate the p-value for this t-statistic. Based on the p-value, reject or fail to reject the null hypothesis. A.5.2 Comparing one mean with a threshold - one sample t-test Let us return to our class of students. Figure A.14 shows a histogram of the changes in their test scores following their course of study. We shall call this change DIFF: Figure A.14: Changes in exam scores following a course of study. Do these provide evidence that the course has improved scores, or not? We see that some students did score worse in the test following the course than in the test preceding it, but a majority have improved their score. It seems from this chart that the course of study has helped the students, on the whole, but to check that this improvement is significant we carry out a test. The test we will use is called a “one sample” t-test. One sample, because we have just one sample of data, in this case the differences in the students scores, and we compare these with a null, or reference value, which in this case is zero. In other words, the test will tell us whether there is evidence from the data to reject the null value, which is that the course made no difference to the students’ scores. The mean value \\(\\bar{y}\\) of DIFF is 0.862 and the standard deviation s of DIFF is 0.838. The number of students is n is 30, so the degrees of freedom df is 29. The standard error in the mean is \\(\\frac{s}{\\sqrt{n}} = 0.153\\). Hence the t-statistic, the number of standard errors of the mean from the null prediction of 0 (in this case) is 0.862 / 0.153 = 5.64. Look at the plot of a t-distribution for 30 degrees of the freedom (that for 29 degrees of freedom will be very similar to that) in Figure A.9 above. What proportion of the area under the curve, do you think, is more than 5.64 standard errors away from 0? Most of it, some of it, or practically none of it? You can see that this distance is so far away from zero that the area under the curve that is that far or further from zero is effectively zero. We interpret this as meaning there is almost zero probability that we would have got this data if the null hypothesis were true. This probability is what we call a p-value. In particular, the p-value for this one sample t-test is well below 0.05, in fact p &lt; 0.001 so we can confidently reject the null hypothesis and conclude that in general, students’ score did improve following their course of study. (We infer the direction of change from the fact that the mean difference is positive, and also from the range of values contained within the confidence interval.) If we were to do this test in R, this is the output we would get: ## ## One Sample t-test ## ## data: df$DIFF ## t = 5.6364, df = 29, p-value = 4.337e-06 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.5493389 1.1750558 ## sample estimates: ## mean of x ## 0.8621973 A.5.3 Comparing two means - two-sample test A.5.3.1 A two-sample test for a difference Suppose we hypothesise that male and female squirrels differ in body mass. 50 squirrels of each sex are measured, and the body masses of each are recorded. Histograms and qq-plots of the data are shown in Figure A.15 below: Figure A.15: Histograms and qq-plots for small data sets that are not normally distributed, but are in fact skewed to the right, with a tail of points going out to high values. Note the characteristic curvature in the qq-plots for data that does this. These histograms, especially for males, do not look very symmetrical. Both distributions are skewed to the right. The effect of this is that the few squirrels with particularly large body masses will greatly increase the means of the samples and in doing so suggest that the whole body mass distribution is greater than it is in reality. This is reflected in the qqplots for both sexes, which are distinctly curved. The data are clearly not normally distributed. As it stands, we should not use a t-test to decide whether the data are drawn from the same distribution. We could instead either use a non-parametric test for a difference, such as a Wilcoxon-Mann-Whitney test that does not demand that the data follow a particular distribution, or we could attempt one of a number of possible transformations of the data, such as taking the natural log of the body mass, in the hope that this would rein in the long tail towards high values and achieve a more symmetric distribution. (We could also attempt some version of a generalised linear model, but that is a topic we will not explore here.) We show the result of doing this in Figure A.16 below Figure A.16: Histograms and qq-plots for the same data, but after it has been log transformed. That’s much better. Here is a summary of the log(mass) values for each sample: sex Mean log (mass) SD log (mass) FEMALE -0.688 0.245 MALE -0.579 0.325 Our null hypothesis \\(H_0\\) is that the two samples are drawn from populations with the same mean mass, and the alternate hypothesis \\(H_1\\) is that the populations from which they are drawn do not have the same mean mass: \\[ \\begin{align*} \\text{H}_0&amp;:\\quad\\mu_\\text{A}=\\mu_\\text{B}\\quad\\text{or}\\quad\\mu_\\text{A}-\\mu_\\text{B}=0\\\\ \\text{H}_\\text{1}&amp;:\\quad\\mu_\\text{A}\\neq\\mu_\\text{B}\\quad\\text{or}\\quad\\mu_\\text{A}-\\mu_\\text{B}\\neq 0 \\end{align*} \\] To find out whether or not we can reject the null hypothesis, we can use the fact that the difference between the means \\(\\mu_A-\\mu_B\\) is itself a random variable that follows a t-distribution and which under the null hypothesis has a mean of zero. What we need to do is calculate the t-statistic in this case, which is the difference between the sample means divided by the standard error of that difference^*^. This turns out to be the square root of the sum of the squared standard errors of the individual means, a result that comes from the fact that the variance of the difference between two independent random variables is the sum of the variances of each of the variables, and a standard error is the square root of a variance. Handy to know, but you don’t have to. In practice, most people will simply use the R function t.test() without worrying about the details, and so can you, if you want to. With these data we find that the absolute difference between the sample means is \\(|-0.688 - -0.579| = 0.109\\) and that the standard error of the difference between the means is \\[ \\text{SE} = \\sqrt{\\frac{0.245^2}{50} + \\frac{0.325^2}{50}} = 0.0576 \\] Hence the test statistic \\(t\\) in this case is \\(\\frac{0.109}{0.0576} = 1.90\\) The number of degrees of freedom in this case is \\(n_\\text{A}+n_\\text{B}-2=50+50-2=98\\). A t-distribution with 98 degrees of freedom is indistinguishable from a normal distribution and you may recall from above that the threshold \\(t\\) (or \\(z\\), it makes no difference with this many degrees of freedom) value in a two-sided test for a p-value of 0.05 or less is 1.96. Our value here is less than that and hence we conclude that there is no evidence from the data at the 5% signficance level that males and females have different body masses. If we do this test in R, this is the output we get: ## ## Two Sample t-test ## ## data: males_log_mass and females_log_mass ## t = 1.8976, df = 98, p-value = 0.0607 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.005001637 0.223448674 ## sample estimates: ## mean of x mean of y ## -0.5785825 -0.6878061 Note that in this output from R we are given the t-statistic, the number of degrees of freedom and the p-value, on the basis of which we decide whether or not to reject the null hypothesis which is that the samples are drawn from populations with the same mean value. Note too that we are given a 95% confidence interval. This is the range of values that would capture the true value of the difference between the means of our samples 95% of the time, if we repeated the trial many times over. If this range encompasses zero, then we have no grounds, at the 95% confidence level, to think that the difference between the means among the populations is anything other than zero. Whenever this confidence interval does encompass zero, as it does here, the p-value will also be greater than 0.05, as it is here. A.5.3.2 Erroneous use of a t-test As a word of caution about using a t-test on data that does not meet the criteria required, look what we get if we did the t-test on the untransformed masses, which we have seen above to have skewed, non-normal distributions: ## ## Two Sample t-test ## ## data: males_mass and females_mass ## t = 2.1662, df = 98, p-value = 0.03272 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.006107238 0.139492762 ## sample estimates: ## mean of x mean of y ## 0.5908 0.5180 This gives a t-statistic of 2.17 and p-value of 0.03, on the basis of which we would erroneously reject the null hypothesis that males and female squirrels on average have the same body mass. We would be wrong to think that this output gave us grounds to do that, since the data do not meet the criteria required for a t-test, and hence we should not believe the output if we use one. A.5.4 Alternatives to the t-test In fact, in this situation, while you should not use a t-test on the untransformed data these do not meet the criteria of constant variance, normally distributed residuals etc, using it on the transformed data is just one of the alternatives you could reasonably use. One other is to use an appropriate non-parametric test on the data. Such tests, while less powerful than parametric tests like the t-test do not require that the data follow any particular distribution. They do not use the actual values of individual data points but instead use their ranks: 1 for the smallest, 2 for the second smallest and so on. An example of such a test that we could use here is the Wilcoxon rank sum / Mann-Whitney test, implemented in R using the funcion wilcox.test() ## ## Wilcoxon rank sum test with continuity correction ## ## data: males_mass and females_mass ## W = 1488, p-value = 0.1014 ## alternative hypothesis: true location shift is not equal to 0 The outcome of this test, which it would be perfectly valid to use here, is that there is no evidence from the data that the weights of male and female squirrels are different. A second alternative is to use one of a powerful set of tests known as Generalised Linear Models. We will not delve into those here. A.6 Size effects vs hypothesis testing. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
